# 7.1 Building Performant Pages with Large Datasets - Study Notes

## Overview

Building performant pages that handle large volumes of posts, users, or metadata efficiently requires careful query design, memory management, and understanding of backend limitations.

**Key Reference:** [WP_Query - WordPress Developer Resources](https://developer.wordpress.org/reference/classes/wp_query/)

---

## 1. Optimizing Loops and Queries for Large Datasets

### Limiting Query Results

**Always Set Limits:**
```php
// ❌ BAD - No limit, loads all posts
$query = new WP_Query( array(
	'post_type' => 'product',
) );

// ✅ GOOD - Limited results
$query = new WP_Query( array(
	'post_type' => 'product',
	'posts_per_page' => 20,
	'no_found_rows' => true, // Skip pagination count query
) );
```

**Using Fields Parameter:**
```php
// ✅ GOOD - Only get IDs, not full post objects
$query = new WP_Query( array(
	'post_type' => 'product',
	'fields' => 'ids', // Reduces memory usage significantly
	'posts_per_page' => 100,
) );

// Process IDs individually
foreach ( $query->posts as $post_id ) {
	$post = get_post( $post_id );
	// Process post
}
```

### Efficient Loop Design

**Avoid Loading Full Objects:**
```php
// ❌ BAD - Loads all post data into memory
$posts = get_posts( array(
	'post_type' => 'product',
	'posts_per_page' => -1, // Gets ALL posts
) );

// ✅ GOOD - Process in batches
function process_large_dataset( $batch_size = 50 ) {
	$paged = 1;

	do {
		$query = new WP_Query( array(
			'post_type' => 'product',
			'posts_per_page' => $batch_size,
			'paged' => $paged,
			'fields' => 'ids',
			'no_found_rows' => true,
		) );

		if ( $query->have_posts() ) {
			foreach ( $query->posts as $post_id ) {
				// Process individual post
				process_post( $post_id );
			}
		}

		$paged++;
		wp_reset_postdata();
	} while ( $query->have_posts() );
}
```

---

## 2. Applying Efficient Code Design to Reduce Memory and CPU Load

### Memory-Efficient Patterns

**Process Data Incrementally:**
```php
// ✅ GOOD - Process one at a time
function export_large_dataset() {
	$query = new WP_Query( array(
		'post_type' => 'product',
		'posts_per_page' => 1,
		'fields' => 'ids',
	) );

	while ( $query->have_posts() ) {
		$query->the_post();
		$post_id = get_the_ID();

		// Process and output immediately
		process_and_output( $post_id );

		// Clear memory
		wp_cache_delete( $post_id, 'posts' );
	}

	wp_reset_postdata();
}
```

**Avoid N+1 Query Problems:**
```php
// ❌ BAD - N+1 queries
$products = get_posts( array( 'post_type' => 'product' ) );
foreach ( $products as $product ) {
	$meta = get_post_meta( $product->ID, 'price', true ); // Query per post
}

// ✅ GOOD - Batch meta retrieval
$product_ids = wp_list_pluck( $products, 'ID' );
$meta_values = get_post_meta( $product_ids[0], 'price', true ); // Single query with update_post_meta_cache
update_post_meta_cache( $product_ids );
```

### CPU Optimization

**Use Specific Queries:**
```php
// ❌ BAD - Broad query, then filter in PHP
$all_posts = get_posts( array( 'post_type' => 'post' ) );
$featured = array_filter( $all_posts, function( $post ) {
	return get_post_meta( $post->ID, 'featured', true ) === 'yes';
} );

// ✅ GOOD - Filter in database query
$featured = get_posts( array(
	'post_type' => 'post',
	'meta_query' => array(
		array(
			'key' => 'featured',
			'value' => 'yes',
			'compare' => '=',
		),
	),
	'posts_per_page' => 20,
) );
```

**Cache Expensive Operations:**
```php
function get_expensive_calculation( $post_id ) {
	$cache_key = 'expensive_calc_' . $post_id;
	$result = wp_cache_get( $cache_key );

	if ( false === $result ) {
		// Expensive calculation
		$result = perform_expensive_calculation( $post_id );
		wp_cache_set( $cache_key, $result, '', 3600 );
	}

	return $result;
}
```

---

## 3. Recognising Backend Limits and Planning Around Them

### Memory Limits

**PHP Memory Configuration:**
```php
// Check current memory limit
$memory_limit = ini_get( 'memory_limit' );

// Increase for specific operations
ini_set( 'memory_limit', '256M' );

// Monitor memory usage
$memory_before = memory_get_usage();
// ... operation ...
$memory_used = memory_get_usage() - $memory_before;
error_log( "Memory used: " . size_format( $memory_used ) );
```

**Memory-Efficient Processing:**
```php
function process_with_memory_limit( $max_memory_mb = 128 ) {
	$memory_limit = $max_memory_mb * 1024 * 1024;
	$batch_size = 10;

	do {
		$memory_before = memory_get_usage();

		$query = new WP_Query( array(
			'posts_per_page' => $batch_size,
			'fields' => 'ids',
		) );

		foreach ( $query->posts as $post_id ) {
			process_post( $post_id );
		}

		// Clear memory
		wp_reset_postdata();
		wp_cache_flush_group( 'posts' );

		$memory_used = memory_get_usage() - $memory_before;

		if ( $memory_used > $memory_limit ) {
			$batch_size = max( 1, $batch_size - 1 );
		}

	} while ( $query->have_posts() );
}
```

### Execution Time Limits

**Handle Timeouts:**
```php
// Set execution time limit
set_time_limit( 300 ); // 5 minutes

// Process with time tracking
function process_with_timeout( $max_time = 240 ) {
	$start_time = time();
	$paged = 1;

	do {
		// Check if we're approaching timeout
		if ( ( time() - $start_time ) > $max_time ) {
			// Schedule continuation via WP-Cron
			wp_schedule_single_event( time() + 60, 'continue_processing', array( $paged ) );
			break;
		}

		$query = new WP_Query( array(
			'posts_per_page' => 50,
			'paged' => $paged,
		) );

		// Process batch
		process_batch( $query->posts );

		$paged++;
	} while ( $query->have_posts() );
}
```

### Database Connection Limits

**Optimise Query Patterns:**
```php
// ❌ BAD - Multiple separate queries
foreach ( $post_ids as $post_id ) {
	$post = get_post( $post_id ); // Separate query
	$meta = get_post_meta( $post_id ); // Separate query
}

// ✅ GOOD - Batch queries
$posts = get_posts( array(
	'post__in' => $post_ids,
	'posts_per_page' => count( $post_ids ),
) );
update_post_meta_cache( $post_ids );
update_post_term_cache( $post_ids );
```

---

## 4. Large User Datasets

### Efficient User Queries

**Limit User Queries:**
```php
// ❌ BAD - Load all users
$users = get_users();

// ✅ GOOD - Paginated user queries
$users = get_users( array(
	'number' => 50,
	'offset' => 0,
) );

// Process in batches
function process_users_in_batches() {
	$offset = 0;
	$number = 100;

	do {
		$users = get_users( array(
			'number' => $number,
			'offset' => $offset,
			'fields' => 'ID', // Only get IDs
		) );

		foreach ( $users as $user_id ) {
			process_user( $user_id );
		}

		$offset += $number;
	} while ( count( $users ) === $number );
}
```

---

## 5. Large Meta Datasets

### Optimising Meta Queries

**Efficient Meta Queries:**
```php
// ❌ BAD - Query all meta, then filter
$all_meta = get_post_meta( $post_id );
$price = $all_meta['price'][0];

// ✅ GOOD - Query specific meta key
$price = get_post_meta( $post_id, 'price', true );

// For multiple posts
$post_ids = array( 1, 2, 3, 4, 5 );
foreach ( $post_ids as $post_id ) {
	$meta = get_post_meta( $post_id, 'price', true );
}

// Better - Use meta_query in WP_Query
$query = new WP_Query( array(
	'post__in' => $post_ids,
	'meta_query' => array(
		array(
			'key' => 'price',
			'compare' => 'EXISTS',
		),
	),
) );
```

**Avoid Large Meta Tables:**
```php
// Consider custom tables for large datasets
// Instead of storing in wp_postmeta
// Use custom table with proper indexes
global $wpdb;
$results = $wpdb->get_results( $wpdb->prepare(
	"SELECT post_id, meta_value
	FROM {$wpdb->prefix}custom_meta
	WHERE meta_key = %s
	AND post_id IN (" . implode( ',', array_map( 'intval', $post_ids ) ) . ")",
	'price'
) );
```

---

## Exam Tips

### Key Points to Remember

1. **Always Set Limits:**
   - Use `posts_per_page` with reasonable values
   - Never use `-1` for unlimited
   - Use `no_found_rows => true` when pagination not needed

2. **Memory Efficiency:**
   - Use `fields => 'ids'` when possible
   - Process data in batches
   - Clear caches between batches
   - Monitor memory usage

3. **Query Optimization:**
   - Filter in database, not PHP
   - Use batch queries instead of loops
   - Update meta/term caches in bulk
   - Avoid N+1 query patterns

4. **Backend Limits:**
   - Monitor PHP memory limits
   - Handle execution time limits
   - Plan for database connection limits
   - Use WP-Cron for long-running tasks

5. **Large Datasets:**
   - Process incrementally
   - Use pagination or cursor-based navigation
   - Consider custom tables for very large datasets
   - Cache expensive calculations

---

## Additional Resources

- [WP_Query - WordPress Developer Resources](https://developer.wordpress.org/reference/classes/wp_query/)
- [get_posts() - WordPress Developer Resources](https://developer.wordpress.org/reference/functions/get_posts/)
- [Performance - WordPress Developer Resources](https://developer.wordpress.org/advanced-administration/performance/optimization/)
- Core Files:
  - `wp-includes/class-wp-query.php` - Query class implementation
  - `wp-includes/query.php` - Query helper functions
